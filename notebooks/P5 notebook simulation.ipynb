{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba7c7d6-5c00-47a6-98d8-a05f212b7794",
   "metadata": {},
   "source": [
    "## Maintenance Contract – Cluster Monitoring & Retraining\n",
    "\n",
    "This notebook implements a “maintenance contract” for the customer segmentation model (M0). Its goal is to monitor when the clustering model degrades over time and to define a clear retraining cadence based on both cluster stability and feature drift.\n",
    "\n",
    "**Key objectives:**\n",
    "1. **Simulate Cluster Drift**  \n",
    "   - Use the Adjusted Rand Index (ARI) to compare the original K-Means segmentation (M0) against fresh re-fits at weekly snapshots.  \n",
    "   - Identify the week when ARI falls below our 0.8 threshold (≈90 % label agreement), triggering a model retrain.\n",
    "\n",
    "2. **Assess Feature Stability**  \n",
    "   - For each continuous feature (e.g. recency, log spend), run two-sample KS tests between week 0 and each subsequent week.  \n",
    "   - For binary flags (e.g. returning‐customer), apply chi-square tests to detect any shifts in proportions.\n",
    "\n",
    "3. **Define Maintenance Cadence**  \n",
    "   - Combine ARI decline and feature-drift signals to recommend a retraining interval (e.g. every 12 weeks).  \n",
    "   - Highlight which features drive most of the drift—and which segments require the most urgent attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d48e31-9a79-4336-8084-ecca3b7da438",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da1c87a3-60f0-4ff1-bbe0-5d32a2ad48b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/nataschajademinnitt/Documents/5. Data Analysis/segmenting_customers\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Set-up environment\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\")\n",
    "os.chdir('/Users/nataschajademinnitt/Documents/5. Data Analysis/segmenting_customers/')\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c8de28-573b-4083-8044-2bbb20ce558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 96478 entries, 0 to 96477\n",
      "Data columns (total 4 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   customer_unique_id        96478 non-null  object \n",
      " 1   order_purchase_timestamp  96478 non-null  object \n",
      " 2   order_id                  96478 non-null  object \n",
      " 3   m_price                   96478 non-null  float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"./data/processed/df_maintenance.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b8b63-637b-464f-be9a-36fb5774ff64",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa37d5-770a-4d07-86f7-b3cf8085f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ari_drift(df, features, model, scaler, k, max_weeks=24, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Simulate ARI-based maintenance drift.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for w in range(0, min(df['weeks_since'].max(), max_weeks) + 1):\n",
    "        snap = df[df['weeks_since'] >= w]\n",
    "        Xw_s = scaler.transform(snap[features].values)\n",
    "        C_init = model.predict(Xw_s)\n",
    "        C_new = KMeans(n_clusters=k, random_state=42).fit_predict(Xw_s)\n",
    "        ari = adjusted_rand_score(C_init, C_new)\n",
    "        results.append({'weeks_since': w, 'ari': ari})\n",
    "        if ari < threshold:\n",
    "            break\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff6021-34d6-4270-87c1-d4d4dda8aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ari_drift(df_ari, threshold=0.8, xtick_step=2,\n",
    "                   xlabel='Weeks since initial fit (T0)',\n",
    "                   ylabel='Adjusted Rand Index',\n",
    "                   title='Model Validity Over Time',\n",
    "                   outpath=None):\n",
    "    \"\"\"\n",
    "    Plot ARI drift over time.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(df_ari['weeks_since'], df_ari['ari'], marker='o', linestyle='-')\n",
    "    ax.axhline(threshold, color='red', linestyle='--', label=f'ARI = {threshold}')\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    max_week = df_ari['weeks_since'].max()\n",
    "    ax.set_xticks(range(0, max_week + 1, xtick_step))\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    if outpath:\n",
    "        plt.savefig(outpath, dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89ad15-2fbb-49c0-8985-6af6639b923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_drift_tests(df, features, max_week=12):\n",
    "    \"\"\"\n",
    "    Perform KS tests for continuous feature drift.\n",
    "    \"\"\"\n",
    "    ks_results = []\n",
    "    baseline = df[df['weeks_since'] == 0]\n",
    "    for feat in features:\n",
    "        data0 = baseline[feat]\n",
    "        for w in range(1, min(df['weeks_since'].max(), max_week) + 1):\n",
    "            dataw = df[df['weeks_since'] == w][feat]\n",
    "            if len(dataw) < 10:\n",
    "                continue\n",
    "            stat, pval = ks_2samp(data0, dataw)\n",
    "            ks_results.append({\n",
    "                'feature': feat,\n",
    "                'week': w,\n",
    "                'ks_stat': stat,\n",
    "                'pvalue': pval\n",
    "            })\n",
    "    return pd.DataFrame(ks_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9a236-f818-4f5e-88bb-dac142b955f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_drift_tests(df, feature, max_week=12):\n",
    "    \"\"\"\n",
    "    Perform chi-square tests for binary feature drift.\n",
    "    \"\"\"\n",
    "    base = df[df['weeks_since'] == 0][feature].value_counts().sort_index().reindex([0,1], fill_value=0)\n",
    "    chi_results = []\n",
    "    for w in range(1, min(df['weeks_since'].max(), max_week) + 1):\n",
    "        comp = (df[df['weeks_since'] == w][feature]\n",
    "                .value_counts().sort_index()\n",
    "                .reindex([0,1], fill_value=0))\n",
    "        table = pd.DataFrame({'week0': base, f'week{w}': comp})\n",
    "        chi2, p, dof, expected = chi2_contingency(table.values)\n",
    "        chi_results.append({'week': w, 'chi2_stat': chi2, 'p_value': p})\n",
    "    return pd.DataFrame(chi_results).set_index('week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddea195-a0a2-4a25-9a51-5ea648aa8fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_drift(\n",
    "    df_ks,\n",
    "    df_chi,\n",
    "    max_week=12,\n",
    "    save_fig=False,\n",
    "    fig_path=\"./plots/feature_drift.png\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot KS p-values for continuous features and chi-square p-values for one binary feature.\n",
    "    \"\"\"\n",
    "    weeks = range(1, max_week+1)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 6))\n",
    "\n",
    "    # Top: KS tests for continuous features\n",
    "    for feat in df_ks['feature'].unique():\n",
    "        sub = df_ks[df_ks['feature'] == feat]\n",
    "        ax1.plot(sub['week'], sub['pvalue'], marker='o', label=feat)\n",
    "    ax1.axhline(0.05, color='red', linestyle='--', label='α = 0.05')\n",
    "    ax1.set_ylabel('KS p-value')\n",
    "    ax1.set_title('Continious Feature Drift (KS Test)')\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    # Bottom: Chi-square test for binary feature\n",
    "    ax2.plot(\n",
    "        df_chi.index, df_chi['p_value'],\n",
    "        marker='s', color='tab:blue', label='binary feature'\n",
    "    )\n",
    "    ax2.axhline(0.05, color='red', linestyle='--', label='α = 0.05')\n",
    "    ax2.set_xlabel('Weeks since initial training (T0)')\n",
    "    ax2.set_ylabel('Chi-square p-value')\n",
    "    ax2.set_title('Binary Feature Drift (Test χ²)')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.xticks(weeks)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_fig:\n",
    "        os.makedirs(os.path.dirname(fig_path), exist_ok=True)\n",
    "        fig.savefig(fig_path, dpi=150)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c816b2f-9432-48a6-9373-d136a78d83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_monthly_ari(\n",
    "    df,\n",
    "    features,\n",
    "    best_k,\n",
    "    base_end,\n",
    "    n_windows=7,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute ARI between the base-month model (M0) and sliding monthly models (M1..M6).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain timestamp_col and feature columns.\n",
    "    timestamp_col : str\n",
    "        Name of the datetime column.\n",
    "    features : list of str\n",
    "        Feature columns for clustering.\n",
    "    best_k : int\n",
    "        Number of clusters for KMeans.\n",
    "    base_end : pd.Timestamp\n",
    "        End date for M0 window (exclusive).\n",
    "    n_windows : int\n",
    "        Number of monthly windows to evaluate (including M0).\n",
    "    random_state : int\n",
    "        Seed for KMeans and subsampling.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with columns ['model','end_date','ari'] for M1..M6.\n",
    "    \"\"\"\n",
    "    \n",
    "    models = []\n",
    "    scalers = []\n",
    "    end_dates = []\n",
    "    \n",
    "    # 1) Fit models M0..M6 on rolling 12-month windows\n",
    "    for i in range(n_windows):\n",
    "        end = base_end + relativedelta(months=i)\n",
    "        start = end - relativedelta(years=1)\n",
    "        mask = (df['order_purchase_timestamp'] >= start) & (df['order_purchase_timestamp'] < end)\n",
    "        window = df.loc[mask, features]\n",
    "        X = window.values\n",
    "        \n",
    "        scaler = StandardScaler().fit(X)\n",
    "        Xs = scaler.transform(X)\n",
    "        \n",
    "        km = KMeans(n_clusters=best_k, random_state=random_state).fit(Xs)\n",
    "        \n",
    "        scalers.append(scaler)\n",
    "        models.append(km)\n",
    "        end_dates.append(end)\n",
    "    \n",
    "    # 2) Compute ARI between M0.predict and Mx.predict on the same X\n",
    "    results = []\n",
    "    for i in range(1, n_windows):\n",
    "        end = end_dates[i]\n",
    "        start = end - relativedelta(years=1)\n",
    "        window = df.loc[(df['order_purchase_timestamp'] >= start) & (df['order_purchase_timestamp'] < end), features]\n",
    "        X = window.values\n",
    "        \n",
    "        labels0 = models[0].predict(scalers[0].transform(X))\n",
    "        labels_i = models[i].predict(scalers[i].transform(X))\n",
    "        \n",
    "        ari = adjusted_rand_score(labels0, labels_i)\n",
    "        results.append({\n",
    "            'model': f'M{i}',\n",
    "            'end_date': end,\n",
    "            'ari': ari\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ef9f7-4b0b-412b-9647-3fc59382ac25",
   "metadata": {},
   "source": [
    "## Creatings DFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d62a3-b605-4581-a8bd-f828349a76c6",
   "metadata": {},
   "source": [
    "## ARI revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1178e30-6acf-470a-8c3a-fc4c7755dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biweekly_ari(\n",
    "    df,\n",
    "    features,\n",
    "    best_k,\n",
    "    lead_time_weeks=12,\n",
    "    step_weeks=2,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Define M0 end at (max_date - lead_time_weeks).\n",
    "    2) Fit M0 on the prior 12 months.\n",
    "    3) In bi-weekly steps, slide the end date forward by `step_weeks`:\n",
    "       - Retrain Mi on the year ending at that new end date.\n",
    "       - Compute ARI between M0.predict and Mi.predict on their shared customers.\n",
    "    Returns a DataFrame of ARI vs. window-end.\n",
    "    \"\"\"\n",
    "    # ensure datetime\n",
    "    df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'])\n",
    "    max_date   = df['order_purchase_timestamp'].max()\n",
    "    \n",
    "    # 1) M0 end and training window\n",
    "    m0_end = max_date - relativedelta(weeks=lead_time_weeks)\n",
    "    m0_start = m0_end - relativedelta(years=1)\n",
    "    win0 = df[(df['order_purchase_timestamp'] >= m0_start) & (df['order_purchase_timestamp'] < m0_end)]\n",
    "    \n",
    "    # agg features on win0\n",
    "    feat0 = (\n",
    "        win0\n",
    "        .groupby('customer_unique_id')\n",
    "        .agg(\n",
    "            last_purchase = ('order_purchase_timestamp', 'max'),\n",
    "            frequency     = ('order_id','nunique'),\n",
    "            m_price       = ('m_price','sum')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    feat0['recency']     = (m0_end - feat0['last_purchase']).dt.days\n",
    "    feat0['f_returning'] = (feat0['frequency'] > 1).astype(int)\n",
    "    feat0['m_price_log'] = np.log1p(feat0['m_price'])\n",
    "    feat0 = feat0[['customer_unique_id'] + features].set_index('customer_unique_id')\n",
    "    \n",
    "    # scale & fit M0\n",
    "    X0  = feat0.values\n",
    "    scaler0 = StandardScaler().fit(X0)\n",
    "    km0     = KMeans(n_clusters=best_k, random_state=random_state).fit(scaler0.transform(X0))\n",
    "    \n",
    "    # prepare loop over future ends\n",
    "    results = []\n",
    "    n_steps = lead_time_weeks // step_weeks  # 12/2 = 6\n",
    "    for i in range(1, n_steps+1):\n",
    "        end_i = m0_end + relativedelta(weeks=step_weeks*i)\n",
    "        start_i = end_i - relativedelta(years=1)\n",
    "        win_i = df[(df['order_purchase_timestamp'] >= start_i) & (df['order_purchase_timestamp'] < end_i)]\n",
    "        \n",
    "        feat_i = (\n",
    "            win_i\n",
    "            .groupby('customer_unique_id')\n",
    "            .agg(\n",
    "                last_purchase = ('order_purchase_timestamp', 'max'),\n",
    "                frequency     = ('order_id','nunique'),\n",
    "                m_price       = ('m_price','sum')\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        feat_i['recency']     = (end_i - feat_i['last_purchase']).dt.days\n",
    "        feat_i['f_returning'] = (feat_i['frequency'] > 1).astype(int)\n",
    "        feat_i['m_price_log'] = np.log1p(feat_i['m_price'])\n",
    "        feat_i = feat_i[['customer_unique_id'] + features].set_index('customer_unique_id')\n",
    "        \n",
    "        # intersect customers\n",
    "        common = feat0.index.intersection(feat_i.index)\n",
    "        X0_c = feat0.loc[common].values\n",
    "        Xi_c = feat_i.loc[common].values\n",
    "        \n",
    "        # fit Mi\n",
    "        scaleri = StandardScaler().fit(Xi_c)\n",
    "        km_i    = KMeans(n_clusters=best_k, random_state=random_state).fit(scaleri.transform(Xi_c))\n",
    "        \n",
    "        # predict & ARI\n",
    "        l0 = km0.predict(scaler0.transform(X0_c))\n",
    "        li = km_i.predict(scaleri.transform(Xi_c))\n",
    "        ari = adjusted_rand_score(l0, li)\n",
    "        \n",
    "        results.append({\n",
    "            'step':       i,\n",
    "            'end_date':   end_i,\n",
    "            'weeks_from_M0': step_weeks * i,\n",
    "            'n_common':   len(common),\n",
    "            'ari':        ari\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d6bf5-bfa2-4a58-a103-d4669e5a5ebb",
   "metadata": {},
   "source": [
    "### Cluster drift\n",
    "\n",
    "The Adjusted Rand Index (ARI) is used to determine cluster drift in order to determine an appropriate maintenance cadence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b81a739-702e-47ba-a2b2-965304563a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   step            end_date  weeks_from_M0  n_common  ari\n",
      "0     1 2018-06-20 15:00:37              2     63789 0.93\n",
      "1     2 2018-07-04 15:00:37              4     62563 0.88\n",
      "2     3 2018-07-18 15:00:37              6     60834 0.80\n",
      "3     4 2018-08-01 15:00:37              8     59132 0.73\n",
      "4     5 2018-08-15 15:00:37             10     57356 0.63\n",
      "5     6 2018-08-29 15:00:37             12     55618 0.57\n"
     ]
    }
   ],
   "source": [
    "# ARI from Tmax - 12 weeks\n",
    "df_ari_bi = biweekly_ari(\n",
    "    df,\n",
    "    features=['recency','f_returning','m_price_log'],\n",
    "    best_k=4,\n",
    "    lead_time_weeks=12,\n",
    "    step_weeks=2\n",
    ")\n",
    "print(df_ari_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee89f6-7046-4f71-9a46-d4f127fbcd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI from Tmax - 24 weeks\n",
    "df_ari_bi = biweekly_ari(\n",
    "    df,\n",
    "    features=['recency','f_returning','m_price_log'],\n",
    "    best_k=4,\n",
    "    lead_time_weeks=8,\n",
    "    step_weeks=2\n",
    ")\n",
    "print(df_ari_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc27c7-9d57-4db1-aa4a-a9d7c0c1eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise ARI\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(df_ari_bi['weeks_from_M0'], df_ari_bi['ari'], marker='o', linestyle='-')\n",
    "ax.axhline(0.8, color='red', linestyle='--', label=f'ARI = 0.8')\n",
    "ax.set_xlabel('Weeks Since M0 Max')\n",
    "ax.set_ylabel('ARI')\n",
    "ax.set_title('M0 validity over time (RFM)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./plots/maintenance/M0_ARI.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab76ec-57a0-4003-a5a9-c179978a435b",
   "metadata": {},
   "source": [
    "Thresholds defined between 0 and 1 trained on a 3 year model. justify with scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cae789-e896-4f26-a4b4-1ec60d16b756",
   "metadata": {},
   "source": [
    "Test with different t0 (normally testing forwards ratehr than backwards)\n",
    "Use recency curve to decided on an overall time frame (1 year for seaosnality or 18 months)\n",
    "automoize every 2 weeks to trigger retraining. \n",
    "\n",
    "Min date and max date need to be fixed: 12 months (train features on this time frame)\n",
    "Add one week T0 - 1 week\n",
    "\n",
    "M0 trains on 12 months, M1 = 12 months + 1 week\n",
    "\n",
    "max = august 2018\n",
    "m0 = feb 2017 - feb 2018 (trained feb to feb and predicted on Mx)\n",
    "m1 = march 2017 - march 2018 (delta 1 month)\n",
    "m2 = april 2017 - april 2018\n",
    "... M6 (fit predict for each Mx)\n",
    "can an older model be as effective as the new timeframe?\n",
    "\n",
    "ARI = weeks M1 comparing clusters of M0 and Mx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a4d4c2-3efd-4d05-bbc8-a39ce9382a0b",
   "metadata": {},
   "source": [
    "### Feature drift\n",
    "Kolmogorov–Smirnov tests are used to assess how continious features (recency and m_price_log) shift over time, and a chi-square test is used to assess binary features (f_returning) shifts over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f76f6-d55a-4e4c-a666-666c4a4361cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi2 drift\n",
    "df_chi = chi2_drift_tests(df, 'f_returning', max_week=12)\n",
    "\n",
    "# KS drift\n",
    "df_ks  = ks_drift_tests(df, ['recency','m_price_log'], max_week=12)\n",
    "\n",
    "plot_feature_drift(df_ks, df_chi,\n",
    "                   max_week=12,\n",
    "                   save_fig=True,\n",
    "                   fig_path=\"./plots/maintenance/feature_drift_rfm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd448f-96ee-4ba6-a6a7-b4729637242e",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "* Recency: Every week back, the “recency” distribution is completely different, which is no surprise as “recency” is “days since last order”. As such, recency will always drive some of your ARI decay.\n",
    "* Price: Week 1’s price distribution is statistically the same. Changes in what people spend start to change the clusters after ~2 weeks.\n",
    "* f_returning is extremely stable over time. The fraction of customers who’ve returned at least once stays essentially constant across all snapshots. It’s a “safe” feature for segmentation. Since it doesn’t drift, it won’t erode cluster definitions. No need for frequent retraining just for this flag."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
